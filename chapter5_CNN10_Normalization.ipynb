{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1871fbf2",
   "metadata": {},
   "source": [
    "# 批量归一化\n",
    "\n",
    "后面的层训练较快\n",
    "\n",
    "数据在底部，底部训练较慢\n",
    "\n",
    "底部层变化，所有都跟着变（为什么？）\n",
    "\n",
    "我们可以在学习底部层的时候避免变化顶部层吗？\n",
    "\n",
    "### 批量归一化的思想\n",
    "\n",
    "\n",
    "固定小批量中的均值和方差，然后学习出适合的偏移和方差\n",
    "\n",
    "可以加速**收敛速度**，但一般不改变模型精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe45b20",
   "metadata": {},
   "source": [
    "我觉得Batch Normalization 有用还是因为解决数值稳定性问题。\n",
    "\n",
    "另外，有各种各样的normalization，可以看一下：\n",
    "\n",
    "![normalization](./img/normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a9dbe",
   "metadata": {},
   "source": [
    "# coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d7e07f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/Atari/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43ed8e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving 和 momentum 用于做平滑处理，因为batch的算出来的mean和var噪声太大\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    if not torch.is_grad_enabled(): # predicting\n",
    "        X_hat = (X-moving_mean)/torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4) # MLP or conv2D\n",
    "        if len(X.shape) == 2: # 二维的话，第一维度是batch， 第二维度是全联接\n",
    "            # dim = 0: 按行处理，处理特征\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X-mean)**2).mean(dim=0)\n",
    "        else:\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = ((X-mean)**2).mean(dim=(0,2,3),keepdim=True)\n",
    "        X_hat = (X-mean) / torch.sqrt(var + eps)\n",
    "        moving_mean = momentum * moving_mean + (1 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1-momentum) * var\n",
    "    # 最后再使用学习的两个参数进行还原（这两个参数的范围我们可以自己控制）\n",
    "    Y = gamma * X_hat + beta\n",
    "    \n",
    "    return Y, moving_mean.data, moving_var.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e76c0c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "            \n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        \n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "        X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        \n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "448c433d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = torch.rand(size=(100, 32))\n",
    "Y = test_X.mean(dim=0,keepdim=True)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d28e00",
   "metadata": {},
   "source": [
    "## 应用在LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06fb1077",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "        nn.Conv2d(1, 6, kernel_size=5, padding=2), # 6 * 28 * 28\n",
    "        BatchNorm(6, 4),\n",
    "        nn.Sigmoid(), \n",
    "        nn.AvgPool2d(2, stride=2),# 6 * 14 * 14\n",
    "        nn.Conv2d(6, 16, kernel_size=5),# 16 * 10 * 10\n",
    "        BatchNorm(16, 4),\n",
    "        nn.Sigmoid(),\n",
    "        nn.AvgPool2d(2, stride=2),# 16 * 5 * 5\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(16*5*5, 120),\n",
    "        BatchNorm(120, 2),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(120, 84),\n",
    "        BatchNorm(84, 2),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(84, 10)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "388f8489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "\n",
    "batch_size = 256\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    trans = [transforms.ToTensor()]\n",
    "    # 如果有resize，先做resize，然后再转换到tensor\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root='./data/', train=True, transform=trans, download=False)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\"./data\", train=False, transform=trans, download=False)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=8), \n",
    "           data.DataLoader(mnist_test, batch_size, shuffle=True, num_workers=8))\n",
    "  \n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5e2bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "loss: 0.47621700167655945\n",
      "loss: 0.37128904461860657\n",
      "loss: 0.32932472229003906\n",
      "loss: 0.430637925863266\n",
      "loss: 0.29687103629112244\n",
      "loss: 0.5083425641059875\n",
      "loss: 0.283053994178772\n",
      "loss: 0.2738705575466156\n",
      "loss: 0.36384066939353943\n",
      "loss: 0.10750596970319748\n"
     ]
    }
   ],
   "source": [
    "def train(net, train_iter, test_iter, num_epochs, lr):\n",
    "    def init_weights(m):\n",
    "        if type(m)==nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            \n",
    "    net.apply(init_weights)\n",
    "    print('begin training')\n",
    "    \n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = net(X)\n",
    "            L = loss(y_hat, y)\n",
    "            L.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f'loss: {L.sum()}')\n",
    "\n",
    "lr = 0.9\n",
    "num_epochs = 10\n",
    "train(net, train_iter, test_iter, num_epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45116669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data accurancy: 0.8999\n",
      "train data accurancy: 0.9168333333333333\n"
     ]
    }
   ],
   "source": [
    "def accurancy(y, y_hat):\n",
    "    if len(y_hat) > 1 and len(y_hat[0]) > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat == y\n",
    "    return cmp.sum().item()\n",
    "\n",
    "# 测试集上的效果\n",
    "net.eval()\n",
    "\n",
    "sum_accu = 0\n",
    "for X, y in test_iter:\n",
    "    y_hat = net(X)\n",
    "    sum_accu += accurancy(y, y_hat)\n",
    "    \n",
    "print(f'test data accurancy: {sum_accu / 10000}')\n",
    "\n",
    "\n",
    "\n",
    "# 在训练集上的效果\n",
    "sum_accu = 0\n",
    "for X, y in train_iter:\n",
    "    y_hat = net(X)\n",
    "    sum_accu += accurancy(y, y_hat)\n",
    "    \n",
    "\n",
    "print(f'train data accurancy: {sum_accu / 60000}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7967333",
   "metadata": {},
   "source": [
    "## 在LeNet上使用 group norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba7027c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_use_group_normalization = torch.nn.Sequential(\n",
    "        nn.Conv2d(1, 6, kernel_size=5, padding=2), # 6 * 28 * 28\n",
    "        nn.GroupNorm(2, 6), # 6个channel 分成 2个group\n",
    "        nn.Sigmoid(), \n",
    "        nn.AvgPool2d(2, stride=2),# 6 * 14 * 14\n",
    "        nn.Conv2d(6, 16, kernel_size=5),# 16 * 10 * 10\n",
    "        nn.GroupNorm(2, 16),\n",
    "        nn.Sigmoid(),\n",
    "        nn.AvgPool2d(2, stride=2),# 16 * 5 * 5\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(16*5*5, 120),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(120, 84),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(84, 10)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

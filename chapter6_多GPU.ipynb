{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e383f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "主要的思路就是，\n",
    "\n",
    "1. 将数据拆分，参数复制到每个GPU上\n",
    "2. 将每个GPU上的loss做累加\n",
    "3. 将全部算出来的梯度再分散给每个GPU的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e389ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/Atari/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, num_gpus, batch_size, lr):\n",
    "    # 这一步是关键，将网络参数复制到所有的device\n",
    "    # 之后的训练都一样，不用专门去讲梯度复制到每个device\n",
    "    # torch会在背后帮你做这件事情\n",
    "    net = nn.DataParallel(net, device_ids=devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a423d58",
   "metadata": {},
   "source": [
    "# 分布式计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9967231",
   "metadata": {},
   "source": [
    "数据放在分布式文件系统上\n",
    "\n",
    "多个参数服务器应对多个GPU机器，每个GPU都会得到全部的模型全部的参数。\n",
    "\n",
    "多台GPU机器之间的通讯\n",
    "\n",
    "网络通讯是瓶颈。\n",
    "\n",
    "batch的问题很多都是因为数据多样性"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
